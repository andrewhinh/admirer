{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlH0lCOttCs5"
   },
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPRHaeetRnT"
   },
   "source": [
    "# Lab 05: Troubleshooting & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bry3Hr-PcgDs"
   },
   "source": [
    "### What You Will Learn\n",
    "\n",
    "- Practices and tools for testing and linting Python code in general: `black`, `flake8`, `precommit`, `pytests` and `doctests`\n",
    "- How to implement tests for ML training systems in particular\n",
    "- What a PyTorch training step looks like under the hood and how to troubleshoot performance bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs0LXXlCU6Ix"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkQiK7lkgeXm"
   },
   "source": [
    "If you're running this notebook on Google Colab,\n",
    "the cell below will run full environment setup.\n",
    "\n",
    "It should take about three minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVx7C7H0PIZC"
   },
   "outputs": [],
   "source": [
    "lab_idx = 5\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sThWeTtV6fL_"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "full_width = True\n",
    "frame_height = 720  # adjust for your screen\n",
    "\n",
    "if full_width:  # if we want the notebook to take up the whole width\n",
    "    # add styling to the notebook's HTML directly\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow along with a video walkthrough on YouTube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src=\"https://fsdl.me/2022-lab-05-video-embed\", width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFP8lU4nSg1P"
   },
   "source": [
    "# Linting Python and Shell Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXbdYfFlPhZ-"
   },
   "source": [
    "### Automatically linting with `pre-commit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysqqb2GjvLrz"
   },
   "source": [
    "We want keep our code clean and uniform across developers\n",
    "and time.\n",
    "\n",
    "Applying the cleanliness checks and style rules should be\n",
    "as painless and automatic as possible.\n",
    "\n",
    "For this purpose, we recommend bundling linting tools together\n",
    "and enforcing them on all commits with\n",
    "[`pre-commit`](https://pre-commit.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvqtZChKvLr0"
   },
   "source": [
    "In addition to running on every commit,\n",
    "`pre-commit` separates the model development environment from the environments\n",
    "needed for the linting tools, preventing conflicts\n",
    "and simplifying maintenance and onboarding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0XuIuKOXhJl"
   },
   "source": [
    "This cell runs `pre-commit`.\n",
    "\n",
    "The first time it is run on a machine, it will install the environments for all tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hltYGbpNvLr1"
   },
   "outputs": [],
   "source": [
    "!pre-commit run --all-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLw08gIkvLr1"
   },
   "source": [
    "The output lists all the checks that are run and whether they are passed.\n",
    "\n",
    "Notice there are a number of simple version-control hygiene practices included\n",
    "that aren't even specific to Python, much less to machine learning.\n",
    "\n",
    "For example, several of the checks prevent accidental commits with private keys, large files, \n",
    "leftover debugger statements, or merge conflict annotations in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHEEjb9kvLr1"
   },
   "source": [
    "These linting actions are configured via\n",
    "([what else?](https://twitter.com/charles_irl/status/1446235836794564615?s=20&t=OOK-9NbgbJAoBrL8MkUmuA))\n",
    "a YAML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgXa8BzrvLr2"
   },
   "outputs": [],
   "source": [
    "!cat .pre-commit-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HYc_WbTvLr2"
   },
   "source": [
    "Most of the general cleanliness checks are from hooks built by `pre-commit`.\n",
    "\n",
    "See the comments and links in the `.pre-commit-config.yaml` for more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9rTgRqzvLr2"
   },
   "outputs": [],
   "source": [
    "!cat .pre-commit-config.yaml | grep repos -A 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ptkO7aPvLr2"
   },
   "source": [
    "Let's take a look at the section of the file\n",
    "that applies most of our Python style enforcement with\n",
    "[`flake8`](https://flake8.pycqa.org/en/latest/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALsRKfcevLr3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat .pre-commit-config.yaml | grep \"flake8 python\" -A 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Q0BwQUXbg6"
   },
   "source": [
    "The majority of the style checking behavior we want comes from the\n",
    "`additional_dependencies`, which are\n",
    "[plugins](https://flake8.pycqa.org/en/latest/glossary.html#term-plugin)\n",
    "that extend `flake8`'s list of lints.\n",
    "\n",
    "Notice that we have a `--config` file passed in to the `args` for the `flake8` command.\n",
    "\n",
    "We keep the configuration information for `flake8`\n",
    "separate from that for `pre-commit`\n",
    "in case we want to use additional tools with `flake8`,\n",
    "e.g. if some developers want to integrate it directly into their editor,\n",
    "and so that if we change away from `.pre-commit`\n",
    "but keep `flake8` we don't have to\n",
    "recreate our configuration in a different tool.\n",
    "\n",
    "As much as possible, codebases should strive for single sources of truth\n",
    "and link back to those sources of truth with documentation or comments,\n",
    "as in the last line above.\n",
    "\n",
    "Let's take a look at the contents of `flake8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doC_4WQwvLr3"
   },
   "outputs": [],
   "source": [
    "!cat .flake8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Nq6HnyU0M47"
   },
   "source": [
    "There's a lot here! We'll focus on the most important bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4PiB8CPvLr3"
   },
   "source": [
    "Linting tools in Python generally work by emitting error codes\n",
    "with one or more letters followed by three numbers.\n",
    "The `select` argument picks which error codes we want to check for.\n",
    "Error codes are matched by prefix,\n",
    "so for example `B` matches `BTS101` and\n",
    "`G1` matches `G102` and `G199` but not `ARG404`.\n",
    "\n",
    "Certain codes are `ignore`d in the default `flake8` style,\n",
    "which is done via the `ignore` argument,\n",
    "and we can `extend` the list of `ignore`d codes with `extend-ignore`.\n",
    "For example, we rely on `black` to do our formatting,\n",
    "so we ignore some of `flake8`'s formatting codes.\n",
    "\n",
    "Together, these settings define our project's particular style.\n",
    "\n",
    "But not every file fits this style perfectly.\n",
    "Most of the conventions in `black` and `flake8` come from the style-defining\n",
    "[Python Enhancement Proposal 8](https://peps.python.org/pep-0008/),\n",
    "which exhorts you to \"know when to be inconsistent\".\n",
    "\n",
    "To allow ourselves to be inconsistent when we know we should be,\n",
    "`flake8` includes `per-file-ignores`,\n",
    "which let us ignore specific warnings in specific files.\n",
    "This is one of the \"escape valves\"\n",
    "that makes style enforcement tolerable.\n",
    "We can also `exclude` files in the `pre-commit` config itself.\n",
    "\n",
    "For details on selecting and ignoring,\n",
    "see the [`flake8` docs](https://flake8.pycqa.org/en/latest/user/violations.html)\n",
    "\n",
    "For definitions of the error codes from `flake8` itself,\n",
    "see the [list in the docs](https://flake8.pycqa.org/en/latest/user/error-codes.html).\n",
    "Individual extensions list their added error codes in their documentation,\n",
    "e.g. `darglint` does so\n",
    "[here](https://github.com/terrencepreilly/darglint#error-codes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL0TpyPsvLr4"
   },
   "source": [
    "The remainder are configurations for the other `flake8` plugins that we use to define and enforce the rest of our style.\n",
    "\n",
    "You can read more about each in their documentation:\n",
    "- [`flake8-import-order`](https://github.com/PyCQA/flake8-import-order) for checking imports\n",
    "- [`flake8-docstrings`](https://github.com/pycqa/flake8-docstrings) for docstring style\n",
    "- [`darglint`](https://github.com/terrencepreilly/darglint) for docstring completeness\n",
    "- [`flake8-annotations`](https://github.com/sco1/flake8-annotations) for type annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFsZC0a7vLr4"
   },
   "source": [
    "### Linting via a script and using `shellcheck`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYjpuFwjXkJc"
   },
   "source": [
    "To avoid needing to think about `pre-commit`\n",
    "(was the command `pre-commit run` or `pre-commit check`?)\n",
    "while developing locally,\n",
    "we might put our linters into a shell script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXlLFWmavLr4"
   },
   "outputs": [],
   "source": [
    "!cat tasks/lint.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPxHpRIB3nbw"
   },
   "source": [
    "These kinds of short and simple shell scripts are common in projects\n",
    "of intermediate size.\n",
    "\n",
    "They are useful for adding automation and reducing friction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMuPBpAi2qwl"
   },
   "source": [
    "But these scripts are code,\n",
    "and all code is susceptible to bugs and subject to concerns of style consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQRg3ZqXvLr4"
   },
   "source": [
    "We can't check these scripts with tools that lint Python code,\n",
    "so we include a shell script linting tool,\n",
    "[`shellcheck`](https://www.shellcheck.net/),\n",
    "in our `pre-commit`.\n",
    "\n",
    "More so than checking for correct style,\n",
    "this tool checks for common bugs or surprising behaviors of shells,\n",
    "which are unfortunately numerous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkfhE1srvLr4"
   },
   "outputs": [],
   "source": [
    "script_filename = \"tasks/lint.sh\"\n",
    "!pre-commit run shellcheck --files {script_filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXU9TRrwvLr4"
   },
   "source": [
    "That script has already been tested, so we don't see any errors.\n",
    "\n",
    "Try copying over a script you've written yourself or\n",
    "even from a popular repo that you like\n",
    "(by adding to the notebook directory or by making a cell\n",
    "with `%%writefile` at the top)\n",
    "and test it by changing the `script_filename`.\n",
    "\n",
    "You'd be surprised at the classes of subtle bugs possible in bash!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81MhAL-TvLr5"
   },
   "source": [
    "### Try \"unofficial bash strict mode\" for louder failures in scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSwhs_zUvLr5"
   },
   "source": [
    "Another way to reduce bugs is to use the suggested \"unofficial bash strict mode\" settings by\n",
    "[@redsymbol](https://twitter.com/redsymbol),\n",
    "which appear at the top of the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-j0vSxEvLr5"
   },
   "outputs": [],
   "source": [
    "!head -n 3 tasks/lint.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2iJU5jlvLr5"
   },
   "source": [
    "The core idea of strict mode is to fail more loudly.\n",
    "This is a desirable behavior of scripts,\n",
    "like the ones we're writing,\n",
    "even though it's an undesirable behavior for an interactive shell --\n",
    "it would be unpleasant to be logged out every time you hit an error.\n",
    "\n",
    "`set -u` means scripts fail if a variable's value is `u`nset,\n",
    "i.e. not defined.\n",
    "Otherwise bash is perfectly happy to allow you to reference undefined variables.\n",
    "The result is just an empty string, which can lead to maddeningly weird behavior.\n",
    "\n",
    "`set -o pipefail` means failures inside a pipe of commands (`|`) propagate,\n",
    "rather than using the exit code of the last command.\n",
    "Unix tools are perfectly happy to work on nonsense input,\n",
    "like sorting error messages, instead of the filenames you meant to send.\n",
    "\n",
    "You can read more about these choices\n",
    "[here](http://redsymbol.net/articles/unofficial-bash-strict-mode/),\n",
    "and considerations for working with other non-conforming scripts in \"strict mode\"\n",
    "and for handling resource teardown when scripts error out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1XqsrU_XWWS"
   },
   "source": [
    "# Testing ML Codebases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPNzeq3NYF2W"
   },
   "source": [
    "## Testing Python code with `pytests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq5e_x6gc9Vu"
   },
   "source": [
    "\n",
    "ML codebases are Python first and foremost, so first let's get some Python tests going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DC3GxYz6_R9"
   },
   "source": [
    "At a basic level,\n",
    "we can write functions that `assert`\n",
    "that our code behaves as expected in\n",
    "a given scenario and include it in the same module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rvd-GNwv63W1"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.lit_models.metrics import test_character_error_rate\n",
    "\n",
    "test_character_error_rate??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVB2TsQS5BTq"
   },
   "source": [
    "The standard tool for testing Python code is\n",
    "[`pytest`]((https://docs.pytest.org/en/7.1.x/)).\n",
    "\n",
    "We can use it as a command-line tool in a variety of ways,\n",
    "including to execute these kinds of tests.\n",
    "\n",
    "If passed a filename, `pytest` will look for\n",
    "any classes that start with `Test` or\n",
    "any functions that start with `test_` and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8sQguyJvLr6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pytest text_recognizer/lit_models/metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92tkBCllvLr6"
   },
   "source": [
    "After the results of the tests (pass or fail) are returned,\n",
    "you'll see a report of \"coverage\" from\n",
    "[`codecov`](https://about.codecov.io/).\n",
    "\n",
    "This coverage report tells us which files and how many lines in those files\n",
    "were at touched by the testing suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PllSUe0s5xvU"
   },
   "source": [
    "We do not actually need to provide the names of files with tests in them to `pytest`\n",
    "in order for it to run our tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qOBHJnTZM9x"
   },
   "source": [
    "By default, `pytest` looks for any files named `test_*.py` or `*_test.py`.\n",
    "\n",
    "It's [good practice](https://docs.pytest.org/en/7.1.x/explanation/goodpractices.html#test-discovery)\n",
    "to separate these from the rest of your code\n",
    "in a folder or folders named `tests`,\n",
    "rather than scattering them around the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acjsYTNSvLr6"
   },
   "outputs": [],
   "source": [
    "!ls text_recognizer/tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZQQZUF0vLr6"
   },
   "source": [
    "Let's take a look at a specific example:\n",
    "the tests for some of our utilities around\n",
    "custom PyTorch Lightning `Callback`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oS0xKv1evLr6"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.tests import test_callback_utils\n",
    "\n",
    "\n",
    "test_callback_utils.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lko8msn-vLr7"
   },
   "source": [
    "Notice that we can easily import this as a module!\n",
    "\n",
    "That's another benefit of organizing tests into specialized files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A85FUNv75Fr"
   },
   "source": [
    "The particular utility we're testing\n",
    "here is designed to prevent crashes:\n",
    "it checks for a particular type of error and turns it into a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jl4-DiVe76sw"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.callbacks.util import check_and_warn\n",
    "\n",
    "check_and_warn??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6E0MhduvLr7"
   },
   "source": [
    "Error-handling code is a common cause of bugs,\n",
    "a fact discovered\n",
    "[again and again across forty years of error analysis](https://twitter.com/full_stack_dl/status/1561880960886505473?s=20&t=5OZBonILaUJE9J4ah2Qn0Q),\n",
    "so it's very important to test it well!\n",
    "\n",
    "We start with a very basic test,\n",
    "which does not touch anything\n",
    "outside of the Python standard library,\n",
    "even though this tool is intended to be used\n",
    "with more complex features of third-party libraries,\n",
    "like `wandb` and `tensorboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xx5koQmJvLr7"
   },
   "outputs": [],
   "source": [
    "test_callback_utils.test_check_and_warn_simple??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZe9-JVjvLr7"
   },
   "source": [
    "Here, we are just testing the core logic.\n",
    "This test won't catch many bugs,\n",
    "but when it does fail, something has gone seriously wrong.\n",
    "\n",
    "These kinds of tests are important for resolving a bug:\n",
    "we learn nearly as much from the tests that passed\n",
    "as we did from the tests that failed.\n",
    "If this test has failed, possibly along with others,\n",
    "we can rule out an issue in one of the large external codebases\n",
    "touched in the other tests, saving us lots of time in our troubleshooting.\n",
    "\n",
    "The reasoning for the test is explained in the docstrings, \n",
    "which are close to the code.\n",
    "\n",
    "Your test suite should be as welcoming\n",
    "as the rest of your codebase!\n",
    "The people reading it, for example yourself in six months, \n",
    "are likely upset and in need of some kindness.\n",
    "\n",
    "More practically, we want keep our time to resolve errors as short as possible,\n",
    "and five minutes to write a good docstring now\n",
    "can save five minutes during an outage, when minutes really matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Om9k-uXhvLr7"
   },
   "source": [
    "That basic test is a start, but it's not enough by itself.\n",
    "There's a specific error case that triggered the addition of this code.\n",
    "\n",
    "So we test that it's handled as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjbsb5FvvLr7"
   },
   "outputs": [],
   "source": [
    "test_callback_utils.test_check_and_warn_tblogger??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGAIZTUjvLr7"
   },
   "source": [
    "That test can fail if the libraries change around our code,\n",
    "i.e. if the `TensorBoardLogger` gets a `log_table` method.\n",
    "\n",
    "We want to be careful when making assumptions\n",
    "about other people's software,\n",
    "especially for fast-moving libraries like Lightning.\n",
    "If we test that those assumptions hold willy-nilly,\n",
    "we'll end up with tests that fail because of\n",
    "harmless changes in our dependencies.\n",
    "\n",
    "Tests that require a ton of maintenance and updating\n",
    "without leading to code improvements soak up\n",
    "more engineering time than they save\n",
    "and cause distrust in the testing suite.\n",
    "\n",
    "We include this test because `TensorBoardLogger` getting\n",
    "a `log_table` method will _also_ change the behavior of our code\n",
    "in a breaking way, and we want to catch that before it breaks\n",
    "a model training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsy95KAvvLr7"
   },
   "source": [
    "Adding error handling can also accidentally kill the \"happy path\"\n",
    "by raising an error incorrectly.\n",
    "\n",
    "So we explicitly test the _absence of an error_,\n",
    "not just its presence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRlIOkjmvLr8"
   },
   "outputs": [],
   "source": [
    "test_callback_utils.test_check_and_warn_wandblogger??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osiqpLynvLr8"
   },
   "source": [
    "There are more tests we could build, e.g. manipulating classes and testing the behavior,\n",
    "testing more classes that might be targeted by `check_and_warn`, or\n",
    "asserting that warnings are raised to the command line.\n",
    "\n",
    "But these three basic tests are likely to catch most changes that would break our code here,\n",
    "and they're a lot easier to write than the others.\n",
    "\n",
    "If this utility starts to get more usage and become a critical path for lots of features, we can always add more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dm285JE5vLr8"
   },
   "source": [
    "## Interleaving testing and documentation with `doctests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHWQvgA8vLr8"
   },
   "source": [
    "One function of tests is to build user/reader confidence in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrhiJBXFvLr8"
   },
   "source": [
    "One function of documentation is to build user/reader knowledge in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vu12LDhvLr8"
   },
   "source": [
    "These functions are related. Let's put them together:\n",
    "put code in a docstring and test that code.\n",
    "\n",
    "This feature is part of the\n",
    "Python standard library via the\n",
    "[`doctest` module](https://docs.python.org/3/library/doctest.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmfIOwXd-Qt7"
   },
   "source": [
    "Here's an example from our `torch` utilities.\n",
    "\n",
    "The `first_appearance` function can be used to\n",
    "e.g. quickly look for stop tokens,\n",
    "giving the length of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzURGcD9vLr8"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.lit_models.util import first_appearance\n",
    "\n",
    "\n",
    "first_appearance??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VtYcJ1WvLr8"
   },
   "source": [
    "Notice that in the \"Examples\" section,\n",
    "there's a short block of code formatted as a\n",
    "Python interpreter session,\n",
    "complete with outputs.\n",
    "\n",
    "We can copy and paste that code and\n",
    "check that we get the right outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dj4lNOxJvLr9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "first_appearance(torch.tensor([[1, 2, 3], [2, 3, 3], [1, 1, 1], [3, 1, 1]]), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9AWHFoIvLr9"
   },
   "source": [
    "We can run the test with `pytest` by passing a command line argument,\n",
    "`--doctest-modules`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMaAxv5ovLr9"
   },
   "outputs": [],
   "source": [
    "!pytest --doctest-modules text_recognizer/lit_models/util.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-2_aOUfvLr9"
   },
   "source": [
    "With the\n",
    "[right configuration](https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2022/blob/627dc9dabc9070cb14bfe5bfcb1d6131eb7dc7a8/pyproject.toml#L12-L17),\n",
    "running `doctest`s happens automatically\n",
    "when `pytest` is invoked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "my_keokPvLr9"
   },
   "source": [
    "## Basic tests for data code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj3Bq_j2_A8o"
   },
   "source": [
    "ML code can be hard to test\n",
    "since it involes very heavy artifacts, like models and data,\n",
    "and very expensive jobs, like training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT5OmgrQvLr9"
   },
   "source": [
    "For testing our data-handling code in the FSDL codebase,\n",
    "we mostly just use `assert`s,\n",
    "which throw errors when behavior differs from expectation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bdzn5g4TvLr9"
   },
   "outputs": [],
   "source": [
    "!grep \"assert\" -r text_recognizer/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aTlfu4_vLr-"
   },
   "source": [
    "This isn't great practice,\n",
    "especially as a codebase grows,\n",
    "because we can't easily know when these are executed\n",
    "or incorporate them into\n",
    "testing automation and coverage analysis tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaMTdmbZ_mkW"
   },
   "source": [
    "So it's preferable to collect up these assertions of simple data properties\n",
    "into tests that are run like our other tests.\n",
    "\n",
    "The test below checks whether any data is leaking\n",
    "between training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qx7cxiDdvLr-"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.tests.test_iam import test_iam_data_splits\n",
    "\n",
    "\n",
    "test_iam_data_splits??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16TJwhd1vLr-"
   },
   "source": [
    "Notice that we were able to load the test into the notebook\n",
    "because it is in a module,\n",
    "and so we can run it here as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mArITFkYvLr-"
   },
   "outputs": [],
   "source": [
    "test_iam_data_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4F2uaclvLr-"
   },
   "source": [
    "But we're checking something pretty simple here,\n",
    "so the new code in each test is just a single line.\n",
    "\n",
    "What if we wanted to test more complex properties,\n",
    "like comparing rows or calculating statistics?\n",
    "\n",
    "We'll end up writing more complex code that might itself have subtle bugs,\n",
    "requiring tests for our tests and suffering from\n",
    "\"tester's regress\".\n",
    "\n",
    "This is the phenomenon,\n",
    "named by analogy with\n",
    "[experimenter's regress](https://en.wikipedia.org/wiki/Experimenter%27s_regress)\n",
    "in sociology of science,\n",
    "where the validity of our tests is itself\n",
    "up for dispute only resolvable by testing the tests,\n",
    "but those tests are themselves possibly invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUGT06gdvLr-"
   },
   "source": [
    "We cut this Gordian knot by using\n",
    "a library or framework that is well-tested.\n",
    "\n",
    "We recommend checking out\n",
    "[`great_expectations`](https://docs.greatexpectations.io/docs/)\n",
    "if you're looking for a high-quality data testing tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ5vNsq3vLr-"
   },
   "source": [
    "Especially with data, some tests are particularly \"heavy\" --\n",
    "they take a long time,\n",
    "and we might want to run them\n",
    "on different machines\n",
    "and on a different schedule\n",
    "than our other tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xephcb0LvLr-"
   },
   "source": [
    "For example, consider testing whether the download of a dataset succeeds and gives the right checksum.\n",
    "\n",
    "We can't just use a cached version of the data,\n",
    "since that won't actually execute the code!\n",
    "\n",
    "This test will take\n",
    "as long to run\n",
    "and consume as many resources as\n",
    "a full download of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSN4w2EqvLr-"
   },
   "source": [
    "`pytest` allows the separation of tests\n",
    "into suites with `mark`s,\n",
    "which \"tag\" tests with names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0rScrcXvLr_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pytest --markers | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr5Ca7B0vLr_"
   },
   "source": [
    "We can choose to run tests with a given mark\n",
    "or to skip tests with a given mark, \n",
    "among other basic logical operations around combining and filtering marks,\n",
    "with `-m`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmw-Eb1ZvLr_"
   },
   "outputs": [],
   "source": [
    "!wandb login  # one test requires wandb authentication\n",
    "\n",
    "!pytest -m \"not data and not slow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LuERxOXX_UJ"
   },
   "source": [
    "## Testing training with memorization tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnWLN4lRvLsA"
   },
   "source": [
    "Training is the process by which we convert inert data into executable models,\n",
    "so it is dependent on both.\n",
    "\n",
    "We decouple checking whether the script has a critical bug\n",
    "from whether the data or model code is broken\n",
    "by testing on some basic \"fake data\",\n",
    "based on a utility from `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4NIc3uWvLsA"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.data import FakeImageData\n",
    "\n",
    "\n",
    "FakeImageData.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deN0swwlvLsA"
   },
   "source": [
    "We then test on the actual data with a smaller version of the real model.\n",
    "\n",
    "We use the Lightning `--fast_dev_run` feature,\n",
    "which sets the number of training, validation, and test batches to `1`.\n",
    "\n",
    "We use a smaller version so that this test can run in just a few minutes\n",
    "on a CPU without acceleration.\n",
    "\n",
    "That allows us to run our tests in environments without GPUs,\n",
    "which saves on costs for executing tests.\n",
    "\n",
    "Here's the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4J0_uD9vLsA"
   },
   "outputs": [],
   "source": [
    "!cat training/tests/test_run_experiment.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-7u9zS1vLsA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! ./training/tests/test_run_experiment.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTzfo11KClV3"
   },
   "source": [
    "The above tests don't actaully check\n",
    "whether any learning occurs,\n",
    "they just check\n",
    "whether training runs mechanically,\n",
    "without any errors.\n",
    "\n",
    "We also need a\n",
    "[\"smoke test\"](https://en.wikipedia.org/wiki/Smoke_testing_(software))\n",
    "for learning.\n",
    "For that we recommending checking whether\n",
    "the model can learn the right\n",
    "outputs for a single batch --\n",
    "to \"memorize\" the outputs for\n",
    "a particular input.\n",
    "\n",
    "This memorization test won't\n",
    "catch every bug or issue in training,\n",
    "which is notoriously difficult,\n",
    "but it will flag\n",
    "some of the most serious issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DVSp3aAvLsA"
   },
   "source": [
    "The script below runs a memorization test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DFVVrxpvLsA"
   },
   "source": [
    "It takes up to two arguments:\n",
    "a `MAX`imum number of `EPOCHS` to run for and\n",
    "a `CRITERION` value of the loss to test against.\n",
    "\n",
    "The test passes if the loss is lower than the `CRITERION` value\n",
    "after the `MAX`imum number of `EPOCHS` has passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEhJH0e5vLsB"
   },
   "source": [
    "The important line in this script is the one that invokes our training script,\n",
    "`training/run_experiment.py`.\n",
    "\n",
    "The arguments to `run_experiment` have been tuned for maximum possible speed:\n",
    "turning off regularization, shrinking the model,\n",
    "and skipping parts of Lightning that we don't want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-fFs1xEvLsB"
   },
   "outputs": [],
   "source": [
    "!cat training/tests/test_memorize_iam.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-47tUA_YNGe"
   },
   "source": [
    "If you'd like to see what a memorization run looks like,\n",
    "flip the `running_memorization` flag to `True`\n",
    "and watch the results stream in to W&B.\n",
    "\n",
    "The cell should run in about ten minutes on a commodity GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwTEsZwKvLsB"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "running_memorization = False\n",
    "\n",
    "if running_memorization:\n",
    "    max_epochs = 1000\n",
    "    loss_criterion = 0.05\n",
    "    !./training/tests/test_memorize_iam.sh {max_epochs} {loss_criterion}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPoFCoEcC8SV"
   },
   "source": [
    "# Troubleshooting model speed with the PyTorch Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpbN-Om2Drf-"
   },
   "source": [
    "Testing code is only half the story here:\n",
    "we also need to fix the issues that our tests flag.\n",
    "This is the process of troubleshooting.\n",
    "\n",
    "In this lab,\n",
    "we'll focus on troubleshooting model performance issues:\n",
    "what do to when your model runs too slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZzwELPXvLsD"
   },
   "source": [
    "Troubleshooting deep neural networks for speed is challenging.\n",
    "\n",
    "There are at least three different common approaches,\n",
    "each with an increasing level of skill required:\n",
    "\n",
    "1. Follow best practices advice from others\n",
    "([this @karpathy tweet](https://t.co/7CIDWfrI0J), summarizing\n",
    "[this NVIDIA talk](https://www.youtube.com/watch?v=9mS1fIYj1So&ab_channel=ArunMallya), is a popular place to start) and use existing implementations.\n",
    "2. Take code that runs slowly and use empirical observations to iteratively improve it.\n",
    "3. Truly understand distributed, accelerated tensor computations so you can write code correctly from scratch the first time.\n",
    "\n",
    "For the full stack deep learning engineer,\n",
    "the final level is typically out of reach,\n",
    "unless you're specializing in the model performance\n",
    "part of the stack in particular.\n",
    "\n",
    "So we recommend reaching the middle level,\n",
    "and this segment of the lab walks through the\n",
    "tools that make this easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_yp87UrFZ8M"
   },
   "source": [
    "Because neural network training involves GPU acceleration,\n",
    "generic Python profiling tools like\n",
    "[`py-spy`](https://github.com/benfred/py-spy)\n",
    "won't work, and\n",
    "we'll need tools specialized for tracing and profiling DNN training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yspsYVFGEyZm"
   },
   "source": [
    "In general, these tools are for observing what happens while your code is executing:\n",
    "_tracing_ which operations were happening when and summarizing that into a _profile_ of the code.\n",
    "\n",
    "Because they help us observe the execution in detail,\n",
    "they will also help us understand just what is going on during\n",
    "a PyTorch training step in greater detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqXq2hKuvLsE"
   },
   "source": [
    "To support profiling and tracing,\n",
    "we've added a new argument to `training/run_experiment.py`, `--profile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_GMMViWvLsE"
   },
   "outputs": [],
   "source": [
    "!python training/run_experiment.py --help | grep -A 1 -e \"^\\s*--profile\\s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZldoksHPvLsE"
   },
   "source": [
    "As with experiment management, this relies mostly on features of PyTorch Lightning,\n",
    "which themselves wrap core utilities from libraries like PyTorch and TensorBoard,\n",
    "and we just add a few lines of customization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2iJ0_A6vLsE"
   },
   "outputs": [],
   "source": [
    "!cat training/run_experiment.py | grep args.profile -A 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aw3ppgndvLsE"
   },
   "source": [
    "For more on profiling with Lightning, see the\n",
    "[Lightning tutorial](https://pytorch-lightning.readthedocs.io/en/1.6.1/advanced/profiler.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCAmNW3QEtcD"
   },
   "source": [
    "The cell below runs an epoch of training with tracing and profiling turned on\n",
    "and then saves the results locally and to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4o3ylDgr46F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from text_recognizer.data.base_data_module import DEFAULT_NUM_WORKERS\n",
    "\n",
    "\n",
    "# make it easier to separate these from training runs\n",
    "%env WANDB_JOB_TYPE=profile\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = DEFAULT_NUM_WORKERS  # change this number later and see how the results change\n",
    "gpus = 1  # must be run with accelerator\n",
    "\n",
    "%run training/run_experiment.py --wandb --profile \\\n",
    "  --max_epochs=1 \\\n",
    "  --num_sanity_val_steps=0 --limit_val_batches=0 --limit_test_batches=0 \\\n",
    "  --model_class=ResnetTransformer --data_class=IAMParagraphs --loss=transformer \\\n",
    "  --batch_size={batch_size} --num_workers={num_workers} --precision=16 --gpus=1\n",
    "\n",
    "latest_expt = wandb.run\n",
    "\n",
    "try:  # add execution trace to logged and versioned binaries\n",
    "    folder = wandb.run.dir\n",
    "    trace_matcher = wandb.run.dir + \"/*.pt.trace.json\"\n",
    "    trace_file = glob.glob(trace_matcher)[0]\n",
    "    trace_at = wandb.Artifact(name=f\"trace-{wandb.run.id}\", type=\"trace\")\n",
    "    trace_at.add_file(trace_file, name=\"training_step.pt.trace.json\")\n",
    "    wandb.log_artifact(trace_at)\n",
    "except IndexError:\n",
    "    print(\"trace not found\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePTkS3EqO5tN"
   },
   "source": [
    "We get out a table of statistics in the terminal,\n",
    "courtesy of Lightning.\n",
    "\n",
    "Each row lists an operation\n",
    "and and provides information,\n",
    "described in the column headers,\n",
    "about the time spent on that operation\n",
    "across all the training steps we profiled.\n",
    "\n",
    "With practice, some useful information can be read out from this table,\n",
    "but it's better to start from both a less detailed view,\n",
    "in the TensorBoard dashboard,\n",
    "and a more detailed view,\n",
    "using the Chrome Trace viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzV62f3c7-Bi"
   },
   "source": [
    "## High-level statistics from the PyTorch Profiler in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNPKXkYw8NWd"
   },
   "source": [
    "Let's look at the profiling info in a high-level TensorBoard dashboard, conveniently hosted for us on W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbItwuT88eAV"
   },
   "outputs": [],
   "source": [
    "your_tensorboard_url = latest_expt.url + \"/tensorboard\"\n",
    "\n",
    "print(your_tensorboard_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE_LooMYHFpF"
   },
   "source": [
    "If at any point you run into issues,\n",
    "like the description not matching what you observe,\n",
    "check out one of our example runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za2zybSwIo5C"
   },
   "outputs": [],
   "source": [
    "example_tensorboard_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2022-training/runs/67j1qxws/tensorboard?workspace=user-cfrye59\"\n",
    "print(example_tensorboard_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlrhl1n4HYU6"
   },
   "source": [
    "Once the TensorBoard session has loaded up,\n",
    "we are dropped into the Overview\n",
    "(see [this screenshot](https://pytorch.org/tutorials/_static/img/profiler_overview1.png)\n",
    "for an example).\n",
    "\n",
    "In the top center, we see the **GPU Summary** for our system.\n",
    "\n",
    "In addition to the name of our GPU,\n",
    "there are a few configuration details and top-level statistics.\n",
    "They are (tersely) documented\n",
    "[here](https://github.com/pytorch/kineto/blob/main/tb_plugin/docs/gpu_utilization.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmBhUDgDLhd1"
   },
   "source": [
    "- **[Compute Capability](https://developer.nvidia.com/cuda-gpus)**:\n",
    "this is effectively a coarse \"version number\" for your GPU hardware.\n",
    "It indexes which features are available,\n",
    "with more advanced features being available only at higher compute capabilities.\n",
    "It does not directly index the speed or memory of the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voUgT6zuLyi0"
   },
   "source": [
    "- **GPU Utilization**: This metric represents the fraction of time an operation (a CUDA kernel) is running on the GPU. This is also reported by the `!nvidia-smi` command or in the sytem metrics tab in W&B. This metric will be our first target to increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl-IndtXE4b4"
   },
   "source": [
    "- **[Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/)**:\n",
    "for devices with compute capability of at least 7, you'll see information about how much your execution used DNN-specialized\n",
    "Tensor Cores.\n",
    "If you're running on an older GPU without Tensor Cores,\n",
    "you should consider upgrading.\n",
    "If you're running a more recent GPU but not seeing Tensor Core usage,\n",
    "you should switch to single precision floating point numbers,\n",
    "which Tensor Cores are specialized on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxcUf0bBNXy_"
   },
   "source": [
    "- **Est. SM Efficiency** and **Est. Occupancy** are high-level summaries of the utilization of GPU hardware\n",
    "at a lower level than just whether something is running at all,\n",
    "as in utilization.\n",
    "Unlike utilization, reaching 100% is not generally feasible\n",
    "and sometimes not desirable.\n",
    "Increasing these numbers requires expertise in\n",
    "CUDA programming, so we'll target utilization instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A88pQn4YMMKc"
   },
   "source": [
    "- **Execution Summary**: This table and pie chart indicates\n",
    "how much time within a profiled step\n",
    "was spent in each category.\n",
    "The value for \"kernel\" execution here\n",
    "is equal to the GPU utilization,\n",
    "and we want that number to be as close to 100%\n",
    "as possible.\n",
    "This summary helps us know which\n",
    "other operations are taking time,\n",
    "like memory being copied between CPU and GPU (`memcpy`)\n",
    "or `DataLoader`s executing on the CPU,\n",
    "so we can decide where the bottleneck is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qjW1RlTQRPv"
   },
   "source": [
    "At the very bottom, you'll find a\n",
    "**Performance Recommendation**\n",
    "tab that sometimes suggests specific methods for improving performance.\n",
    "\n",
    "If this tab makes suggestions, you should certainly take them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWY5AhrcRQmJ"
   },
   "source": [
    "For more on using the profiler in TensorBoard,\n",
    "including some of the other, more detailed views\n",
    "available view the \"Views\" dropdown menu, see\n",
    "[this PyTorch tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?highlight=profiler)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQwrPY_H77H8"
   },
   "source": [
    "## Going deeper with the Chrome Trace Viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhwo7fslvLsH"
   },
   "source": [
    "So far, we've seen summary-level information about our training steps\n",
    "in the table from Lightning and in the TensorBoard Overview.\n",
    "These give aggregate statistics about the computations that occurred,\n",
    "but understanding how to interpret those statistics\n",
    "and use them to speed up our networks\n",
    "requires understanding just what is\n",
    "happening in our training step.\n",
    "\n",
    "Fundamentally,\n",
    "all computations are processes that unfold in time.\n",
    "\n",
    "If we want to really understand our training step,\n",
    "we need to display it that way:\n",
    "what operations were occurring,\n",
    "on both the CPU and GPU,\n",
    "at each moment in time during the training step.\n",
    "\n",
    "This information on timing is collected in the trace.\n",
    "One of the best tools for viewing the trace over time\n",
    "is the [Chrome Trace Viewer](https://www.chromium.org/developers/how-tos/trace-event-profiling-tool/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUkZItxYc20A"
   },
   "source": [
    "Let's tour the trace we just logged\n",
    "with an aim to really understanding just\n",
    "what is happening when we call\n",
    "`training_step`\n",
    "and by extension `.forward`, `.backward`, and `optimizer.step`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w9F2UA7Qctg"
   },
   "source": [
    "The Chrome Trace Viewer is built into W&B,\n",
    "so we can view our traces in their interface.\n",
    "\n",
    "The cell below embeds the trace inside the notebook,\n",
    "but you may wish to open it separately,\n",
    "with the \"Open page\" button or by navigating to the URL,\n",
    "so that you can interact with it\n",
    "as you read the description below.\n",
    "Display directly on W&B is also a bit less temperamental\n",
    "than display on W&B inside a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMUs4aby6Rfd"
   },
   "outputs": [],
   "source": [
    "trace_files_url = latest_expt.url.split(\"/runs/\")[0] + f\"/artifacts/trace/trace-{latest_expt.id}/latest/files/\"\n",
    "trace_url = trace_files_url + \"training_step.pt.trace.json\"\n",
    "\n",
    "example_trace_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2022-training/artifacts/trace/trace-67j1qxws/latest/files/training_step.pt.trace.json\"\n",
    "\n",
    "print(trace_url)\n",
    "IFrame(src=trace_url, height=frame_height * 1.5, width=\"100%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNVpGeQtQjMG"
   },
   "source": [
    "> **Heads up!** We're about to do a tour of the\n",
    "> precise details of the tracing information logged\n",
    "> during the execution of the training code.\n",
    "> The only way to learn how to troubleshoot model performance\n",
    "> empirically is to look at the details,\n",
    "> but the details depend on the precise machine being used\n",
    "> -- GPU and CPU and RAM.\n",
    "> That means even within Colab,\n",
    "> these details change from session to session.\n",
    "> So if you don't observe a phenomenon or feature\n",
    "> described in the tour below, check out\n",
    "> [the example trace](https://wandb.ai/cfrye59/fsdl-text-recognizer-2022-training/artifacts/trace/trace-67j1qxws/latest/files/training_step.pt.trace.json)\n",
    "> on W&B while reading through the next section of the lab,\n",
    "> and return to your trace once you understand the trace viewer better at the end.\n",
    "> Also, these are very much bleeding-edge expert developer tools, so the UX and integrations\n",
    "> can sometimes be a bit janky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXMcBhnCgdN_"
   },
   "source": [
    "This trace reveals, in nanosecond-level detail,\n",
    "what's going on inside of a `training_step`\n",
    "on both the GPU and the CPU.\n",
    "\n",
    "Time is on the horizontal axis.\n",
    "Colored bars represent method calls,\n",
    "and the methods called by a method are placed underneath it vertically,\n",
    "a visualization known as an\n",
    "[icicle chart](https://www.brendangregg.com/flamegraphs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67BsNzDfVIeg"
   },
   "source": [
    "Let's orient ourselves with some gross features:\n",
    "the forwards pass,\n",
    "GPU kernel execution,\n",
    "the backwards pass,\n",
    "and the optimizer step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBEFgtRCKqrh"
   },
   "source": [
    "### The forwards pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nYhiWesVMjK"
   },
   "source": [
    "Type in `resnet` to the search bar in the top-right.\n",
    "\n",
    "This will highlight the first part of the forwards passes we traced, the encoding of the images with a ResNet.\n",
    "\n",
    "It should be in a vertical block of the trace that says `thread XYZ (python)` next to it.\n",
    "\n",
    "You can click the arrows next to that tile to partially collapse these blocks.\n",
    "\n",
    "Next, type in `transformerdecoder` to highlight the second part of our forwards pass.\n",
    "It should be at roughly the same height.\n",
    "\n",
    "Clear the search bar so that the trace is in color.\n",
    "Zoom in on the area of the forwards pass\n",
    "using the \"zoom\" tool in the floating toolbar,\n",
    "so you can see more detail.\n",
    "The zoom tool is indicated by a two-headed arrow\n",
    "pointing into and out of the screen.\n",
    "\n",
    "Switch to the \"drag\" tool,\n",
    "represented by a four-headed arrow.\n",
    "Click-and-hold to use this tool to focus\n",
    "on different parts of the timeline\n",
    "and click on the individual colored boxes\n",
    "to see details about a particular method call.\n",
    "\n",
    "As we go down in the icicle chart,\n",
    "we move from a very abstract level in Python (\"`resnet`\", \"`MultiheadAttention`\")\n",
    "to much more precise `cudnn` and `cuda` operations\n",
    "(\"`aten::cudnn_convolution`\", \"`aten::native_layer_norm`\").\n",
    "\n",
    "`aten` ([no relation to the Pharaoh](https://twitter.com/charles_irl/status/1422232585724432392?s=20&t=Jr4j5ZXhV20xGwUVD1rY0Q))\n",
    "is the tensor math library in PyTorch\n",
    "that links to specific backends like `cudnn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq181ybIvLsH"
   },
   "source": [
    "### GPU kernel execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbkWp5aKvLsH"
   },
   "source": [
    "Towards the bottom, you should see a section labeled \"GPU\".\n",
    "The label appears on the far left.\n",
    "\n",
    "Within it, you'll see one or more \"`stream`s\".\n",
    "These are units of work on a GPU,\n",
    "akin loosely to threads on the CPU.\n",
    "\n",
    "When there are colored bars in this area,\n",
    "the GPU is doing work of some kind.\n",
    "The fraction of this bar that is filled in with color\n",
    "is the same as the \"GPU Utilization %\" we've seen previously.\n",
    "So the first thing to visually assess\n",
    "in a trace view of PyTorch code\n",
    "is what fraction of this area is filled with color.\n",
    "\n",
    "In CUDA, work is queued up to be\n",
    "placed into streams and completed, on the GPU,\n",
    "in a distributed and asynchronous manner.\n",
    "\n",
    "The selection of which work to do\n",
    "is happening on the CPU,\n",
    "and that's what we were looking at above.\n",
    "\n",
    "The CPU and the GPU have to work together to coordinate\n",
    "this work.\n",
    "\n",
    "Type `cuda` into the search bar and you'll see these coordination operations happening:\n",
    "`cudaLaunchKernel`, for example, is the CPU telling the GPU what to do.\n",
    "\n",
    "Running the same PyTorch model\n",
    "with the same high level operations like `Conv2d` in different versions of PyTorch,\n",
    "on different GPUs, and even on tensors of different sizes will result\n",
    "in different choices of concrete kernel operation,\n",
    "e.g. different matrix multiplication algorithms.\n",
    "\n",
    "Type `sync` into the search bar and you'll see places where either work on the GPU\n",
    "or work on the CPU needs to await synchronization,\n",
    "e.g. copying data from the CPU to the GPU\n",
    "or the CPU waiting to decide what to do next\n",
    "on the basis of the contents of a tensor.\n",
    "\n",
    "If you see a \"sync\" block above an area\n",
    "where the stream on the GPU is empty,\n",
    "you've got a performance bottleneck due to synchronization\n",
    "between the CPU and GPU.\n",
    "\n",
    "To resolve the bottleneck,\n",
    "head up the icicle chart until you reach the recognizable\n",
    "PyTorch modules and operations.\n",
    "Find where they are called in your PyTorch module.\n",
    "That's a good place to review your code to understand why the synchronization is happening\n",
    "and removing it if it's not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeMPbu_jvLsI"
   },
   "source": [
    "### The backwards pass\n",
    "\n",
    "Type in `backward` into the search bar.\n",
    "\n",
    "This will highlight components of our backwards pass.\n",
    "\n",
    "If you read it from left to right,\n",
    "you'll see that it begins by calculating the loss\n",
    "(`NllLoss2DBackward` in the search bar if you can't find it)\n",
    "and ends by doing a `ConvolutionBackward`,\n",
    "the first layer of the ResNet.\n",
    "It is, indeed, backwards.\n",
    "\n",
    "Like the forwards pass,\n",
    "the backwards pass also involves the CPU\n",
    "telling the GPU which kernels to run.\n",
    "It's typically run in a separate\n",
    "thread from the forwards pass,\n",
    "so you'll see it separated out from the forwards pass\n",
    "in the trace viewer.\n",
    "\n",
    "Generally, there's no need to specifically optimize the backwards pass --\n",
    "removing bottlenecks in the forwards pass results in a fast backwards pass.\n",
    "\n",
    "One reason why is that these two passes are just\n",
    "\"transposes\" of one another,\n",
    "so they share a lot of properties,\n",
    "and bottlenecks in one become bottlenecks in the other.\n",
    "We can choose to optimize either one of the two.\n",
    "But the forwards pass is under our direct control,\n",
    "so it's easier for us to reason about.\n",
    "\n",
    "Another reason is that the forwards pass is more likely to have bottlenecks.\n",
    "The forwards pass is a dynamic process,\n",
    "with each line of Python adding more to the compute graph.\n",
    "Backwards passes, on the other hand, use a static compute graph,\n",
    "the one just defined by the forwards pass,\n",
    "so more optimizations are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWiDw0vCvLsI"
   },
   "source": [
    "### The optimizer step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndfkzEdnvLsI"
   },
   "source": [
    "Type in `Adam.step` to the search bar to highlight the computations of the optimizer.\n",
    "\n",
    "As with the two passes,\n",
    "we are still using the CPU\n",
    "to launch kernels on the GPU.\n",
    "But now the CPU is looping,\n",
    "in Python, over the parameters\n",
    "and applying the ADAM updates rules to each.\n",
    "\n",
    "We now know enough to see that\n",
    "this is not great for our GPU utilization:\n",
    "there are many areas of gray\n",
    "in between the colored bars\n",
    "in the GPU stream in this area.\n",
    "\n",
    "In the time it takes CUDA to multiply\n",
    "thousands of numbers,\n",
    "Python has not yet finished cleaning up\n",
    "after its request for that multiplication.\n",
    "\n",
    "As of writing in August 2022,\n",
    "more efficient optimizers are not a stable part of PyTorch (v1.12), but\n",
    "[there is an unstable API](https://github.com/pytorch/pytorch/issues/68041)\n",
    "and stable implementations outside of PyTorch.\n",
    "The standard implementations are in\n",
    "[in NVIDIA's `apex.optimizers` library](https://nvidia.github.io/apex/optimizers.html),\n",
    "not to be confused with the\n",
    "[Apex Optimizers Project](https://www.apexoptimizers.com/),\n",
    "which is a collection of fitness-themed cheetah NFTs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX0jxeafvLsI"
   },
   "source": [
    "## Take-aways for PyTorch performance bottleneck troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CugD-bK2vLsI"
   },
   "source": [
    "Our goal here was to learn some basic principles and tools for bottlenecking\n",
    "the most common issues and the lowest-hanging fruit in PyTorch code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwHwJkVMHYGA"
   },
   "source": [
    "\n",
    "Here's an overview in terms of a \"host\",\n",
    "generally the CPU,\n",
    "and a \"device\", here the GPU.\n",
    "\n",
    "- The slow-moving host operates at the level of an abstract compute graph (\"convolve these weights with this input\"), not actual numerical computations.\n",
    "- During execution, host's memory stores only metadata about tensors, like their types and shapes. This metadata needed to select the concrete operations, or CUDA kernels, for the device to run.\n",
    "  - Convolutions with very large filter sizes, for example, might use fast Fourier transform-based convolution algorithms, while the smaller filter sizes typical of contemporary CNNs are generally faster with Winograd-style convolution algorithms.\n",
    "- The much beefier device executes actual operations, but has no control over which operations are executed. Its memory\n",
    "stores information about the contents of tensors,\n",
    "not just their metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gntx28p9cBP5"
   },
   "source": [
    "Towards that goal, we viewed the trace to get an understanding of\n",
    "what's going on inside a PyTorch training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKvZGPnkeXvq"
   },
   "source": [
    "Here's what we've means in terms of troubleshooting bottlenecks.\n",
    "\n",
    "We want Python to chew its way through looking up the right CUDA kernel and telling the GPU that's what it needs next\n",
    "before the previous kernel finishes.\n",
    "\n",
    "Ideally, the CPU is actually getting far _ahead_ of execution\n",
    "on the GPU.\n",
    "If the CPU makes it all the way through the backwards pass before the GPU is done,\n",
    "that's great!\n",
    "The GPU(s) are the expensive part,\n",
    "and it's easy to use multiprocessing so that\n",
    "the CPU has other things to do.\n",
    "\n",
    "This helps explain at least one common piece of advice:\n",
    "the larger our batches are,\n",
    "the more work the GPU has to do for the same work done by the CPU,\n",
    "and so the better our utilization will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMztpa-TccH4"
   },
   "source": [
    "We operationalize our desire to never be waiting on the CPU with a simple metric:\n",
    "**100% GPU utilization**, meaning a kernel is running at all times.\n",
    "\n",
    "This is the aggregate metric reported in the systems tab on W&B or in the output of `!nvidia-smi`.\n",
    "\n",
    "You should not buy faster GPUs until you have maxed this out! If you have 50% utilization, the fastest GPU in the world can't give you more than a 2x speedup, and it will more than 2x cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kYBygfScR6z"
   },
   "source": [
    "Here are some of the most common issues that lead to low GPU Utilization, and how to resolve them:\n",
    "1. **The CPU is too weak**.\n",
    "Because so much of the discussion around DNN performance is about GPUs,\n",
    "it's easy when specing out a machine to skimp on the CPUs, even though training can bottleneck on CPU operations.\n",
    "_Resolution_:\n",
    "Use nice CPUs, like\n",
    "[threadrippers](https://www.amd.com/en/products/ryzen-threadripper).\n",
    "2. **Too much Python during the `training_step`**.\n",
    "Python is very slow, so if you throw in a really slow Python operation, like dynamically creating classes or iterating over a bunch of bytes, especially from disk, during the training step, you can end up waiting on a `__init__`\n",
    "that takes longer than running an entire layer.\n",
    "_Resolution_:\n",
    "Look for low utilization areas of the trace\n",
    "and check what's happening on the CPU at that time\n",
    "and carefully review the Python code being executed.\n",
    "3. **Unnecessary Host/Device synchronization**.\n",
    "If one of your operations depends on the values in a tensor,\n",
    "like `if xs.mean() >= 0`,\n",
    "you'll induce a synchronization between\n",
    "the host and the device and possibly lead\n",
    "to an expensive and slow copy of data.\n",
    "_Resolution_:\n",
    "Replace these operations as much as possible\n",
    "with purely array-based calculations.\n",
    "4. **Bottlenecking on the DataLoader**.\n",
    "In addition to coordinating the work on the GPU,\n",
    "CPUs often perform heavy data operations,\n",
    "including communication over the network\n",
    "and writing to/reading from disk.\n",
    "These are generally done in parallel to the forwards\n",
    "and backwards passes,\n",
    "but if they don't finish before that happens,\n",
    "they will become the bottleneck.\n",
    "_Resolution_:\n",
    "Get better hardware for compute,\n",
    "memory, and network.\n",
    "For software solutions, the answer \n",
    "is a bit more complex and application-dependent.\n",
    "For generic tips, see\n",
    "[this classic post by Ross Wightman](https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19)\n",
    "in the PyTorch forums.\n",
    "For techniques in computer vision, see\n",
    "[the FFCV library](https://github.com/libffcv/ffcv)\n",
    "and for techniques in NLP, see e.g.\n",
    "[Hugging Face datasets with Arrow](https://huggingface.co/docs/datasets/about_arrow)\n",
    "and [Hugging Face FastTokenizers](https://huggingface.co/course/chapter6/3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2WYS8bQvLsJ"
   },
   "source": [
    "### Further steps in making DNNs go brrrrrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0wW2_lRKfY1"
   },
   "source": [
    "It's important to note that utilization\n",
    "is just an easily measured metric\n",
    "that can reveal common bottlenecks.\n",
    "Having high utilization does not automatically mean\n",
    "that your performance is fully optimized.\n",
    "\n",
    "For example,\n",
    "synchronization events between GPUs\n",
    "are counted as kernels,\n",
    "so a deadlock during distributed training\n",
    "can show up as 100% utilization,\n",
    "despite literally no useful work occurring.\n",
    "\n",
    "Just switching to \n",
    "double precision floats, `--precision=64`,\n",
    "will generally lead to much higher utilization.\n",
    "The GPU operations take longer\n",
    "for roughly the same amount of CPU effort,\n",
    "but the added precision brings no benefit.\n",
    "\n",
    "In particular, it doesn't make for models\n",
    "that perform better on our correctness metrics,\n",
    "like loss and accuracy.\n",
    "\n",
    "Another useful yardstick to add\n",
    "to utilization is examples per second,\n",
    "which incorporates how quickly the model is processing data examples\n",
    "and calculating gradients.\n",
    "\n",
    "But really,\n",
    "the gold star is _decrease in loss per second_.\n",
    "This metric connects model design choices\n",
    "and hyperparameters with purely engineering concerns,\n",
    "so it disrespects abstraction barriers\n",
    "and doesn't generally lead to actionable recommendations,\n",
    "but it is, in the end, the real goal:\n",
    "make the loss go down faster so we get better models sooner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFzPsplfdo_o"
   },
   "source": [
    "For PyTorch internals abstractly,\n",
    "see [Ed Yang's blog post](http://blog.ezyang.com/2019/05/pytorch-internals/).\n",
    "\n",
    "For more on performance considerations in PyTorch,\n",
    "see [Horace He's blog post](https://horace.io/brrr_intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFx-OhF837Bp"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq6-S6TC38AY"
   },
   "source": [
    "### 🌟 Compare `num_workers=0`  with `DEFAULT_NUM_WORKERS`.\n",
    "\n",
    "One of the most important features for making\n",
    "PyTorch run quickly is the\n",
    "`MultiprocessingDataLoader`,\n",
    "which executes batching of data in a separate process\n",
    "from the forwards and backwards passes.\n",
    "\n",
    "By default in PyTorch,\n",
    "this feature is actually turned off,\n",
    "via the `DataLoader` argument `num_workers`\n",
    "having a default value of `0`,\n",
    "but we set the `DEFAULT_NUM_WORKERS`\n",
    "to a value based on the number of CPUs\n",
    "available on the system running the code.\n",
    "\n",
    "Re-run the profiling cell,\n",
    "but set `num_workers` to `0`\n",
    "to turn off multiprocessing.\n",
    "\n",
    "Compare and contrast the two traces,\n",
    "both for total runtime\n",
    "(see the time axis at the top of the trace)\n",
    "and for utilization.\n",
    "\n",
    "If you're unable to run the profiles,\n",
    "see the results\n",
    "[here](https://wandb.ai/cfrye59/fsdl-text-recognizer-2022-training/artifacts/trace/trace-2eddoiz7/v0/files/training_step.pt.trace.json#f388e363f107e21852d5$trace-67j1qxws),\n",
    "which juxtaposes two traces,\n",
    "with in-process dataloading on the left and\n",
    "multiprocessing dataloading on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D39w0gXAiha"
   },
   "source": [
    "### 🌟🌟 Resolve issues with a file by fixing flake8 lints, then write a test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2i_a5eVeIoA"
   },
   "source": [
    "The file below incorrectly implements and then incorrectly tests\n",
    "a simple PyTorch utility for adding five to every entry of a tensor\n",
    "and then calculating the sum.\n",
    "\n",
    "Even worse, it does it with horrible style!\n",
    "\n",
    "The cells below apply our linting checks\n",
    "(after automatically fixing the formatting)\n",
    "and run the test.\n",
    "\n",
    "Fix all of the lints,\n",
    "implement the function correctly,\n",
    "and then implement some basic tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSon2fB5VVM_"
   },
   "source": [
    "- [`flake8`](https://flake8.pycqa.org/en/latest/user/error-codes.html) for core style\n",
    "- [`flake8-import-order`](https://github.com/PyCQA/flake8-import-order) for checking imports\n",
    "- [`flake8-docstrings`](https://github.com/pycqa/flake8-docstrings) for docstring style\n",
    "- [`darglint`](https://github.com/terrencepreilly/darglint) for docstring completeness\n",
    "- [`flake8-annotations`](https://github.com/sco1/flake8-annotations) for type annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYiRvU4HA84t"
   },
   "outputs": [],
   "source": [
    "%%writefile training/fixme.py\n",
    "import torch\n",
    "from training import run_experiment\n",
    "from numpy import *\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_five_and_sum(tensor):\n",
    "  # this function is not implemented right,\n",
    "  #    but it's supposed to add five to all tensor entries and sum them up\n",
    "  return 1\n",
    "\n",
    "def test_add_five_and_sum():\n",
    "    # and this test isn't right either! plus this isn't exactly a docstring\n",
    "    all_zeros, all_ones = torch.zeros((2, 3)), torch.ones((1, 4, 72))\n",
    "    all_fives = 5 * all_ones\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXJpmvuzT1w0"
   },
   "outputs": [],
   "source": [
    "!pre-commit run black --files training/fixme.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRO-oJfdUrcQ"
   },
   "outputs": [],
   "source": [
    "!cat training/fixme.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jM8NHxVbSEQD"
   },
   "outputs": [],
   "source": [
    "!pre-commit run --files training/fixme.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kj0VMBSndtkc"
   },
   "outputs": [],
   "source": [
    "!pytest training/fixme.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab05_troubleshooting.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
