# first we specify what we're sweeping
# we specify a program to run
program: training/run_experiment.py
# we optionally specify how to run it, including setting default arguments
command:
    - ${env}
    - ${interpreter}
    - ${program}
    - "--wandb"
    - "--auto_select_gpus"                # GPU settings (Need > 1 GPU available)
    - "--strategy"
    - "ddp_find_unused_parameters_false"
    - "--auto_lr_find"                    # Experiment setup
    - "--auto_scale_batch_size"
    - "--limit_test_batches"
    - "0"
    - "--log_every_n_steps"
    - "50"
    - "--max_epochs"
    - "100"
    - "augment_data"                      # First thing to check if training errors occur
    - "True"
    - "--precision"
    - "16"
    - "--num_workers"
    - "1"
    - ${args}  # these arguments come from the sweep parameters below

# and we specify which parameters to sweep over, what we're optimizing, and how we want to optimize it
method: random  # generally, random searches perform well, can also be "grid" or "bayes"
metric:
    name: train/loss
    goal: minimize
parameters:
    top_k:
        max: 1000
        min: 1
    top_p:
        max: 1.00
        min: 0.01
    max_label_length:
        max: 100
        min: 1
    # we can also fix some values, just like we set default arguments
    model_class:
        value: ViT2GPT2
    data_class:
        value: PICa