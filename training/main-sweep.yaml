# first we specify what we're sweeping
# we specify a program to run
program: training/run_experiment.py
# we optionally specify how to run it, including setting default arguments
command:
    - ${env}
    - ${interpreter}
    - ${program}
    - "--wandb"
    - "--limit_test_batches" # Experiment setup
    - "0"
    - "--log_every_n_steps"
    - "50"
    - "--max_epochs"
    - "100"
    - "--augment_data" # First things to check if training errors occur
    - "True"
    - "--num_workers"
    - "1"
    - ${args}  # these arguments come from the sweep parameters below

# and we specify which parameters to sweep over, what we're optimizing, and how we want to optimize it
method: random  # generally, random searches perform well, can also be "grid" or "bayes"
metric:
    name: train/loss
    goal: minimize
parameters:
    batch_size:
        values: [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
    one_cycle_max_lr:
        max: 1.0
        min: 0.000001
    top_k:
        max: 1000
        min: 1
    top_p:
        max: 1.00
        min: 0.01
    max_label_length:
        max: 100
        min: 1
    # we can also fix some values, just like we set default arguments
    gpus:
        value: 1
    model_class:
        value: ViT2GPT2
    data_class:
        value: PICa